{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initializing \n",
    "Eğer tüm weighleri aynı değer ile başlatırsak layerlardaki tüm katmanlarda aynı hesabı yapıcaz.Bacprop ile yine aynı sonucu alıcaklar. Bu şekilde olması demek modelin ne kadar katmanı olursa olsun tum unitlerde aynı değeri alıcak. Breaking simetryi yapamıyacağız. Bu şekilde liner sonuçlardan başka bişey hesaplayamayız.Bizim istediğimiz her bir unit final kararda farklı katkılarda bulunsun. <br>\n",
    "Genellikle 0 ' yakın değerler başlatmak istememizin sebebi sigmoid için konuşursak büyük değerlerin sigmoid(grafiği düşün) de türevi 0 a çok yakın olucak ve gradient neredeyse 0 olur. Güncelleme yapamayız.  \n",
    "\n",
    "### Neden non-linear ?\n",
    "Eğer aktivasyon fonksiyonlarımız liner olursa her bir katmanda liner hesap yapılacak. Liner denklemlerin toplamı finalde yine liner bir sonuç verir . Bizim gerçek hayat problemlerimiz non-linear (doğrusal olarak açıklanmayan) problemlerdir. Bu sebeple, liner aktivasayon fonksiyonları kullanmak liner aktivasyon kullanan tek katmanlı bir ağdan farklı olmayacak. Non liner kullanarak gerçek hayat problemerine yakın sonıçlar üretiriz.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Bias </b>: Modelin ne kadar hata yaptığını ölçer. \n",
    "<b> Varience </b> Modelin tahmin ettiği değerlerin gerçek veriler etrafında ne kadar saçıldığını ölçer. \n",
    "High bias : Modelin basit kaldığı ve mevcut problemin çözümünü içermediğini gösteriri.\n",
    "High Varience : Modelin kompleks olduğunu ve train yapılan seti ezberlediğini gösterir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization Yöntemleri :<br>\n",
    "\n",
    "l1 ,l2 <br>\n",
    "\n",
    "Dropout : Belirlediğimiz oranda bazı neuronları kaldırırz. Bu şekilde hem modeli daha basit hale getirmiş oluruz. Hemde bazı neuronlara çok bağımlılık varsa bunu ortadan kaldırırz. ( Bazı neuronlar tamamen aktif diğerleri neredeyse yok ) Bu sayede çok aktif bir neuron kapatılınca diğerleride modelde etki sahibi olucak . Model weighleri dağıtmak zorunda kalır <br>\n",
    "Augmentation kullanarak veri arttırma yapılabilir <br>\n",
    "Earlly Stopping ile model durdurulabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why we need normalize input ?\n",
    "Normalizasyon işlemi bizim daha hızlı minimuma ulaşmamızı sağlar. .11 Eğer veri setinde bir feauture diğerlerinden daha büyük değer alıyorsa bu final kararda diğerlerinden daha etkili olucak. 2 Hesaplama maliyeti açısından büyük değerlerle işlem yapmak hem memory usage artırıcak hemde daha yavaş olucak <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishin Gradient : Gradientin early layerlarda çok küçük değerler alması, Bu genellikle sigmoidte karşılaştığımız bir problem sigmoidin türevi 0 - 0.5 araısnda değer alır ve bacprob esnasında 0.5 x 0.5 x 0.5 çarpılması sonucu daha küçük değerler elde edilir. Önceki katmanlara gittikçe gradient yok olur .<br>\n",
    "Exploding : Çok büyümesi sonucu weigtleri overshoot olması "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum \n",
    "Momentum katsayısı ile salınımları ile azaltıyoruz.. B  = 0.9 (Geçmiş 10 gradient e göre karar ver). Weighted averega yönteminde olduğu gibi yukarı asağı salınım azalır direkt minumua doğru yönelim olur. B(beta) büyük çok daha önceki gradientleri işleme kat minumum geçebilir .\n",
    "beta küçüj hızımızı artırmayacak \n",
    "\n",
    "#### RMSPROP\n",
    "beta değeri ile sdw hesaplanır. bu hesaba göre dw nin küçük değerleri için sdw küçük değer alır  weightin w - lrxdw/sdw küçültülmesi daha az olur . sdw nin büyük değerleri için yani büyük weighler için daha küçük güncellemeler ile daha hızlı converge etmeye çalışılır. \n",
    "\n",
    "#### Adam \n",
    "RMSPROP VE momentumun birlikte kullanılması \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr decay \n",
    "learning rate yavaşça küçülterek eppoch sayısı arttıkça daha küçük adımlarla minumuma yaklaşırız. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "İnputu normalize etmek öğrenmeyi hızlandırır. \n",
    "Belirli bi katman için normalizasyon yapmak ; Bir katmanda bulunan weightlerin her epochta aynı dağılımdan gelmesini isteriz. Bu sonraki adımlarda yakınsamamızı iyileştirir. Daha stabil bir aktarım sağlarız. Sonraki katmanda daha hızlı eğitim olması salanır"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOFTMAX (Multi Class)\n",
    "Propability of given x P(cat|x)\n",
    "Softmax katmanında biz here çıkış için prob hesaplarız bu 2 çıkış için düşünürsek. kedi ve köpek tahmin ettiimizde 0.7 kedi 0.3 köpek probu vericek ve daha sonra hatanın hesaplanmasında da labela göre 1-.7 0-.03 heasplanır ve backprob yapılr <br>\n",
    "beklenene çıktı [1 0] output = [0.7 0.3] yixlog(yî) ama logyî düşürmek bunun için yi artırılmalı \n",
    "<b> hardmax : </b> burada softmaxta en büyük olan değer 1 diğerleri 0 alır "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Orthoganlizm : \n",
    "hatayı düzeltmek için tune etme işlemimiz. <br>\n",
    "Avoidable bias : Human level performansa çok yakın bir değer aldıysak bunu ihmal edebiliriz. Bunu yerine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning : \n",
    "Daha önce belirli bir taskta eğitilmiş ve genel olarak özellik çıkarmada başarılı modelin ağırlıklarını alarak o task için kullandığı son katmanı yada eğitmek istediğimiz kadar katmanı sökerek kendi problemimiz çözecek şekilde modeli kullanmak . <br>\n",
    "Az veri olduğu durumlarda kullanmak mantıklıdır. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multitask Learning \n",
    "Aynu anda birden fazla cost fonksiyonun optimize edilmesi (otonom araçlar) softmaxtan farklı olrak bir image birden çok çıkıtıs olabilir  <br>\n",
    "#### END TO END LEARNING\n",
    "Geleneksel yöntemlerde özellikler çıkartılarak bunları modele feedleriz. Konuşma tanıma probleminde ses --- features -- phonemes -- word -- transcript <br>\n",
    "End to end yaklaşımda datayı modele veririz ve onun özellik çıkarmasını bekleriz. Çok data varsa kullanışlı daha az elle düzenleme gerekicek , bırak data konuşsun yaklaşımı Dezavantaj : Çok data ,kullanışlı olacak bazı özellikleri veremiyorz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networklerde biz genelde imageleri kullanırız. Imagelerin üzerinde gezecek filtreler oluştururuz. Ve bu filtreler imageler ile conv işlemine girer. Bu sayede filtreler ile imagelerden özellikler çıkartırız. Horizontal,vertical,diagnol özellikler çıkartılır. <br> Conv işleminde filtreler image üzerinde gezerken işlemden geçen image in boyu küçülür. Bu küçülme image_size - filtersize + 1 olarak hesaplanır <br>\n",
    "<b>Padding : </b> Bu yöntemle filtreleme sonucunda imagin boyunun küçülmesini istemiyorsak kullanırız. Belirtiğimiz oranda imagein etrafına 0 lardan padding oluşturur. <b>valid = no padding ---- same == padding yap</b> n+2p-f+1 ile hesaplanır.<br>\n",
    "<b>Stride :</b> imagenin üzerinde gezicek filtrenin ne kayacağını belirtiriz. (n+2p-f)/s +1 ile imagenin boyu hesaplanır. <br>\n",
    "Filtrelerin boyutları farklı olabilir ama tercih edilmez. Aynı anda birden fazla filtre çalışır. Farklı özellikleri tespit ederler <br>\n",
    "Filtre imagein üzerinde gezer yeni image oluştururlur bias eklenir aktivasyon fonksiyonuna verilir. Daha sonra kaçtane filtre varsa o kadar kanallı bir image oluştururlur . 3x3x3 10 tane filtre olsaydı bir katmanda 3x3x3 => 27 + 1 => 28 x 10 => 280 parmatre olur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POOLING LAYERS \n",
    "CNN lerde boyut küçültmek daha hızlı işlem yapmak ve bazı özelliklerin daha belirginleşmesini sağlamak için pooling layerlar kullanılır .<br>\n",
    "### MAX POOLING \n",
    "Kullanılan filtre (2,2) image üzeirnde gezer ve her gezdiği yerdeki maximum değeri yeni image oluşturarak yazar. imagein boyutu yarıya düşürülür .stride default olarak 2 dir . <br>\n",
    "### AVERAGE POOLING \n",
    "Filtre image üzerinde yine gezer ama bu sefer değerlerin ortalamasını alır. \n",
    "\n",
    "Pooling ile 28x28x3 image >>>>> 14x14x3 olur ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neden conv kullanılır ? \n",
    "Conv kullanımında en önemli iki avantaj parametre sharing ve bağlantı seyrekliğidir. <br>\n",
    "<b>Parametre Sharing</b> : Resmin belli bir noktasında filtrenin tespit ettiği özellik resmin başka yerlerinde de varsa aynı şekilde tespit edilir. Bu sayede aynı parametreler ile resmin farklı yerleri tespit edilmiş olur . Bir filtre dikey kenarları tespit ediyorsa resmin altındaki ve üstündeki dikey kenarlarda aynı filtre kullanılıcak ekstra filtreye ihtiyaç yok.<br>\n",
    "<b> Balantı seyreklii (Sparsity of connection ) </b> : Output değerleri filtrenin boyutu kadar alandan etkilenir. Ekstra bilgilere ve değişikliklere maruz kalmaz. Bu sayede overfitin önüne geçer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET\n",
    "#### Residual Block:\n",
    "Residual bloklar kullarak bilgiyi ileri katmanlara iletebiliyoruz. an de bulunan değerler hem sonraki katmana hemde 2 3 katman sonrasına iletiliyor.Teorik olarak model derinleştikçe modelin representiol capacity yani bilgi çıkarması artıyor.Ancak en büyük problemler model derinleştikçe vanishin gradient probleminin ortaya çıkması ve hesaplama maliyetinin artmasıdır. Daha derin ağlar oluşturabilmek içn \n",
    "Residual blocklarda bir nörondaki aktivasyon ileri katmanlara göndeirliyor ve model derinleştikçe geçmişteki bilgiliyide yanında götürüyor. \n",
    "Bu sayede daha derin ağlar oluşturmamıza olanak sağlanıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK IN NETWORK\n",
    "Network in networklerde 1x1xchannel filtrelerle imagein üzerinde geziyoruz bu sayede imagein kanallarını tek bir pixele sıkıştırarak ve non-liner aktivasyon fonksiyonuna göndererek daha iyi bir representation yapmayı amaçlıyoruz ve aynı zamanda kanal sayısını azaltarak boyutları küçültebiliyoruz. Boyut küçültmek istemezsek bile ağa non-linerity kazandırmış olmamız daha karmaşık problemler çözmemizde yararlı olucak. <br>\n",
    "28x28x192 ye 1x1x192 16 tane filtre uygularsak 28x28x16 elde ederiz <br>\n",
    "### Inception \n",
    "Inception bir çok modülden oluşuyor. her bir modülde farklı sayıda filtreler ve maxpooling bulunuyoren son bunları birleştirerer daha anlamlı özellikler çıkarılmasını bekliyorz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECT DETECTION\n",
    "yaya,araba,motor,arkaplan y = [Pc,bx,by,bh,bw,c1,c2,c3,c4] şeklinde gösterili Pc -- obje varmı yok mu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window \n",
    "Imaging üzeirinde bir pencere ile gezeriz. Nesnenin bulunduğu yere 1 bulunmadğı yere 0 yazarız. Farklı boyutlarda pencereler ile bu tekrar edilir. <br>\n",
    "En büük dezavantajı hesaplama maliyeti, Küçük sliding windowslar iyi ancak çok maliyetli <br>\n",
    "\n",
    "### Convolutional implementation of sliding window\n",
    "Burada temel amaç window ile image üzerinde tek tek gezmek yerine , convneti image üzerinde çalıştıtırız. Tüm image convnetten geçtiinde taranmış olur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Prediction \n",
    "### YOLO ALGORITHM\n",
    "Yolo algoritmasında bir convnet çalıştırıyorz bu çalıştırılan convnett sonucunda elde ettiğimiz activasyonlarda her bir grid içiin obje bulunup bulunmadığı nerede bulunduğu bilgileri bulunuyor. \n",
    "100 x 100 image üzerinde convnet çalıştırdığımızı düşünürsek , en son çıktı olarak 3x3x8 lik bir çıktı ederiz. bu 3 x 3 her bir grid olur 8 ise prediction y=[Pc,bx,by,bh,bw,c1,c2,c3,c4] elde edilir. <br>\n",
    "#### Intersection over union \n",
    "2 bounding boxın benzerliğini ne kadar kesiştiğini ölmek için kullanırız. size_of_kesişim / size_of_birleşim <br>\n",
    "#### Non-max-supression \n",
    "Algorimanın multiple detection yaptığı durumlarda her bir objenin sadece bir kare ile tespit edilmesini sağlamak için kullanırız. <br>\n",
    "Bounding boxlaarın Pc (obje bulunm prob) bakar ve maximumu tespit onunla kesişen ve prob düşük olanları bastırır. Daha sonra başka bölgeler içinde bunu tekrarlar. \n",
    "#### Anchor Boxing \n",
    "Gridde birden fazla obje tespit edildiği durumlarda kullanılır. [Pc,bx,by,bh,bw,c1,c2,c3,c4,Pc,bx,by,bh,bw,c1,c2,c3,c4] şeklinde her bi anchora göre seçim yapılır. <br>\n",
    "### REGION BASE PROPOSAL \n",
    "RCNN : iimage üzerinde bölgeleri tespit etmek için segmentation kullan Segmentation nesne olabilicek bölgeleri tespit ememize olanak sağlar. Ancak hem segmentation hem sliding window kullanmak maliyetli olucak <br>\n",
    "FAST RCNN : Orginal rcnn gibi bölge önerisi yap ama sliding window yerine convnet kullan <br>\n",
    "FASTER RCNN : Segmentation yapmak içinde conv kullan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FACE RECOGNATION\n",
    "### One shot Learning \n",
    "Amaç benzerlik fonksiyonu öğrenmek Önceden eğitilmiş bir convnet kullanılarak bunun herhangi bir katmanında ki aktivasyon kullanılır .\n",
    "Image gönderilir fx hesaplanır daha sonra daha sonra başka biri gönderilir onun için de fx hesaplanı daha sonra bu asıl olması gereken kişi ile similarity function kullanılarak karşılaştırırılı. Benzer olanla benzerliin yüksek olması beklenir. <br>\n",
    "### Triple Loss\n",
    "Amaç anchor ile pozitif arasında ki mesafenin düşürülüp anchor ile negatif arasında mesafenin artırılmasıdır. Belirli bir margin eklenerek Zaten olası olan benzer fotoğrafların daha küçük olma ihtimalini ortadan kaldırmak içn marginle bu mesafelerin daha azaltılıp yada artırılması sağlanır. \n",
    "### Binary\n",
    "yine çıkış katmaında ki aktivasyon binary classification yapmak için sigmoid aktivasyonlu bir denselayera verilebilir. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEURAL STYLE TRANSFER \n",
    "Hedef resmin içeriğini koruyarak referans resmin stilinin aktarımı .<br>\n",
    "Amaç Random olarak başlatılan bir G resmi ile Content(C) Style (S) arasındaki costu düşürmek .<br>\n",
    "J(G) = alpha x J(C,g) + beta x J(S,G) burada similarity hesaplanarak yapılır <br>\n",
    "##### Content Cost\n",
    "Onceden eğitilmiş bir convnet kullanılark bir layerina kadar model alınır. Tercihen ortalarda bir katman daha sonra Content Image ve Generate G image gönderilii aktivasyonlar hesaplanır bunları birbirlerine benzetmeye çalıştığımız için J(C,G) 1/2||aC - aG||^2 minimize edilmeye çalışılır. <br>\n",
    "##### Style Cost\n",
    "Herhangi bir katman seçelim bu katmandaki her bir pixel için önceki kanallar arasında ne derece korelasyon var. Yani 1 sıradaki channelda sarı bölgeleri tespit ediyor. 2 . channeş vertical tespit ediyor..... Bunlar arasında korelasyona bakılır .Düşünüldüğünde büyük aktivasyon değerleri almış yerler orada bulunan özellik aktif olmuş demektir. Aynı şekilde önceki kanallar ile korelasyon hesaplanarak bir Style matrixi elde edilir.<br> G-- Stil matrixi ---   J(C,G) 1/2||GS - GG||^2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECURRENT NEURAL NETWORK\n",
    "Zaman sıralı olarak anlam kazananan veriler ile kullanabileceğimiz bir nn çeşidi. Konuşma tanıma Machine translation, Music Generation işlemlerinde kullanılır. Herhangi bi zamanda unite hem o anki input bilgisi hem bir önceki activasyon gelir. Bu şekilde geçmişten gelen bilgi ileriye taşınır. Kısacası geçmiş bilgilerinde anlamlı olduğu durumlarda kullanışlıdır. <br> \n",
    "Many-to-Many , Many-to-one , one-to-many , many-to,many <br>\n",
    "### Vanishing gradient with RNN :\n",
    "Rnn lerde uzun cümleler yada inputların bilgilerinin taşınmaası zorlaşıyor. Vanishin gradient problemi yanıyor. İlk kelimelerde olan bilgiler sonraki adımlara taşınamıyor. Sonda yaşanan değişiklikler backprop ile önceki kelimeleri değiştiremiyor. Bağlantı zayıf kalıyor. <br>\n",
    "### GATED RECURRENT UNIT (GRU)\n",
    "Uzun mesafeli bağlantılar için ve vanishin gradient probleminde başarılıdır. \n",
    "C memory celli bulunur. update gate ve reset gate bulunur, Update gate geçmiş bilginin ne kadar taşınacağına reset gate geçmiş bilginin ne kadara unutulacağına kara verir. u = 0 olursa güncelleme yapmadan devam et anlamıca gelir. r = 1 olursa tamamen unut anlamına gelir. \n",
    "### LSTM \n",
    "forget gate ,update , output gateleri bulunur. forget = geçmişten gelen bilgiyi unutmak içn kullanılır 0 a yakınsa geçmiş bilgiyi unut 1 e yakınsa hatırla. Update : Mevcut bilgiyi aktarmak için 1 mevcut timestampte bilgiyi aktarmamak için 0 \n",
    "C  = geçmiş bilgi ve şuananı karması şeklinde olur. \n",
    "\n",
    "### Bidirectional \n",
    "Gelecekten ve geçmişten bilgiyi alma \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "### Bag of word \n",
    "textten özellik çıkarmak için kullanılan bir yöntemdir.kelimenin varlığı yada yokluğuna göre bir vektör oluşturulur. [1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1] - keliime geçiyor mu geçmiyor mu. \n",
    "\n",
    "#### Word Embedding nedir? \n",
    "Kelimeleri one hot olarak göstermek yerine embeddingler ile temsil etmemiz. 10000 kelime olduunu düşünürsek 1 kelime 10000 uzunluğunda göstericekti . Ama embedding sayesinde 16  -32 -64 şeklinde gösterilebilir. <br>\n",
    "Büyük corpuslar kullanarak word embeddingleri eğitebiliriz. Embeddingleri yeni tasklerde kullanabiliirz. <br>\n",
    "### Embedding Matrix \n",
    "Toplam kelime sayısı kadar sutun vardır embedding boyutu kadar satır vardır. \n",
    "### Skip gram:<br>\n",
    "### Sequnece to sequnece model  \n",
    "inputu tüm x girdilerini alarak bunları bir rnn den geçirip devamında başka bir rnn aktivasyonu gönderip predict yapmasını sağlama \n",
    "#### Beam Search \n",
    "inpute göre random olarak cümle üretmek yerine toplamda en olası cümleyi terchih etmek. Gredy serch te mevcut inputa göre bir sonraki kelimeyi tahmin ediyordu. Beam search te B parametresi ile olası B sonuç bulunuyor bnular üzerinden devam ediliyo . Cümle uzadıkça beeam searchte prob düşücej bilgisayarın hesaplayamamasına sebep olabilir bu sebeple log(Py|x) hesaplanırz Yüksek B accurate amaa maliyetli Küçük b dah ucuz \n",
    "### Attentation Model \n",
    "Uzun cümleleri transtlate etmek zorlaşır Insanlar uzun cümleleri parçalar halide translate eder Inputtaki bir kelime outpputta ne kadar etkili olucak bunun tespiti için attention model kullamnılır .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
